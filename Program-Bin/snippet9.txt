// High-performance decision tree with SIMD optimizations
class ExtremeDecisionTree {
private:
    struct TreeNode {
        Int64 feature_index;
        Float64 threshold;
        Float64 value;
        unique_ptr<TreeNode> left;
        unique_ptr<TreeNode> right;
        bool is_leaf;
        
        TreeNode(Float64 val = 0.0) : value(val), is_leaf(true) {}
    };
    
    unique_ptr<TreeNode> root;
    Int64 max_depth;
    Int64 min_samples_split;
    Int64 min_samples_leaf;
    PrecisionTimer timer;
    
public:
    ExtremeDecisionTree(Int64 max_depth = 10, Int64 min_samples_split = 2, 
                       Int64 min_samples_leaf = 1)
        : max_depth(max_depth), min_samples_split(min_samples_split),
          min_samples_leaf(min_samples_leaf) {}
    
    void fit(const vector<vector<Float64>>& X, const vector<Float64>& y) {
        timer.start();
        root = buildTree(X, y, 0);
        double elapsed = timer.elapsed();
        
        cout << "=== EXTREME DECISION TREE ===" << endl;
        cout << "Max depth: " << max_depth << endl;
        cout << "Samples: " << X.size() << endl;
        cout << "Features: " << (X.empty() ? 0 : X[0].size()) << endl;
        cout << "Training time: " << elapsed * 1000 << " ms" << endl;
    }
    
    Float64 predict(const vector<Float64>& sample) const {
        return predictSample(sample, root.get());
    }
    
    vector<Float64> predict_batch(const vector<vector<Float64>>& samples) const {
        vector<Float64> predictions(samples.size());
        transform(execution::par_unseq, samples.begin(), samples.end(),
                 predictions.begin(), [this](const auto& sample) { 
                     return predict(sample); 
                 });
        return predictions;
    }
    
    void printTree() const {
        cout << "\nDecision Tree Structure:" << endl;
        printNode(root.get(), 0);
    }
    
private:
    unique_ptr<TreeNode> buildTree(const vector<vector<Float64>>& X, 
                                 const vector<Float64>& y, Int64 depth) {
        auto node = make_unique<TreeNode>();
        
        // Stopping conditions
        if (depth >= max_depth || X.size() <= min_samples_split || 
            isPure(y)) {
            node->value = calculateLeafValue(y);
            return node;
        }
        
        // Find best split
        auto best_split = findBestSplit(X, y);
        if (!best_split.valid) {
            node->value = calculateLeafValue(y);
            return node;
        }
        
        // Split data
        auto [left_X, left_y, right_X, right_y] = splitData(X, y, best_split);
        
        if (left_X.size() < min_samples_leaf || right_X.size() < min_samples_leaf) {
            node->value = calculateLeafValue(y);
            return node;
        }
        
        node->is_leaf = false;
        node->feature_index = best_split.feature_index;
        node->threshold = best_split.threshold;
        node->left = buildTree(left_X, left_y, depth + 1);
        node->right = buildTree(right_X, right_y, depth + 1);
        
        return node;
    }
    
    struct SplitInfo {
        Int64 feature_index;
        Float64 threshold;
        Float64 gain;
        bool valid;
        
        SplitInfo() : valid(false) {}
    };
    
    SplitInfo findBestSplit(const vector<vector<Float64>>& X, 
                          const vector<Float64>& y) {
        SplitInfo best_split;
        best_split.gain = -numeric_limits<Float64>::max();
        
        Int64 n_features = X[0].size();
        Int64 n_samples = X.size();
        
        // Parallel feature evaluation
        #pragma omp parallel for
        for (Int64 feature_idx = 0; feature_idx < n_features; feature_idx++) {
            // Collect feature values and sort indices
            vector<Float64> feature_values(n_samples);
            vector<Int64> indices(n_samples);
            iota(indices.begin(), indices.end(), 0);
            
            for (Int64 i = 0; i < n_samples; i++) {
                feature_values[i] = X[i][feature_idx];
            }
            
            // Sort indices by feature value
            sort(indices.begin(), indices.end(), 
                [&](Int64 a, Int64 b) { return feature_values[a] < feature_values[b]; });
            
            // Evaluate potential splits
            for (Int64 i = 1; i < n_samples; i++) {
                if (feature_values[indices[i]] == feature_values[indices[i-1]]) {
                    continue;
                }
                
                Float64 threshold = (feature_values[indices[i]] + 
                                   feature_values[indices[i-1]]) / 2.0;
                
                Float64 gain = calculateInformationGain(X, y, feature_idx, threshold, indices, i);
                
                #pragma omp critical
                {
                    if (gain > best_split.gain) {
                        best_split.gain = gain;
                        best_split.feature_index = feature_idx;
                        best_split.threshold = threshold;
                        best_split.valid = true;
                    }
                }
            }
        }
        
        return best_split;
    }
    
    Float64 calculateInformationGain(const vector<vector<Float64>>& X, 
                                   const vector<Float64>& y, Int64 feature_idx,
                                   Float64 threshold, const vector<Int64>& indices,
                                   Int64 split_index) {
        // Calculate base entropy
        Float64 base_impurity = calculateVariance(y);
        
        // Split labels
        vector<Float64> left_y, right_y;
        for (Int64 i = 0; i < split_index; i++) {
            left_y.push_back(y[indices[i]]);
        }
        for (Int64 i = split_index; i < static_cast<Int64>(y.size()); i++) {
            right_y.push_back(y[indices[i]]);
        }
        
        if (left_y.empty() || right_y.empty()) return 0.0;
        
        // Calculate weighted impurity
        Float64 left_impurity = calculateVariance(left_y);
        Float64 right_impurity = calculateVariance(right_y);
        
        Float64 n = y.size();
        Float64 weighted_impurity = (left_y.size() / n) * left_impurity + 
                                  (right_y.size() / n) * right_impurity;
        
        return base_impurity - weighted_impurity;
    }
    
    Float64 calculateVariance(const vector<Float64>& values) {
        if (values.empty()) return 0.0;
        Float64 mean = HighPrecisionStats::mean_simd(values);
        Float64 variance = 0.0;
        for (Float64 val : values) {
            variance += (val - mean) * (val - mean);
        }
        return variance / values.size();
    }
    
    bool isPure(const vector<Float64>& y) {
        if (y.empty()) return true;
        Float64 first = y[0];
        return all_of(y.begin(), y.end(), [first](Float64 val) { 
            return abs(val - first) < 1e-10; 
        });
    }
    
    Float64 calculateLeafValue(const vector<Float64>& y) {
        return HighPrecisionStats::mean_simd(y);
    }
    
    tuple<vector<vector<Float64>>, vector<Float64>, 
          vector<vector<Float64>>, vector<Float64>>
    splitData(const vector<vector<Float64>>& X, const vector<Float64>& y, 
              const SplitInfo& split) {
        vector<vector<Float64>> left_X, right_X;
        vector<Float64> left_y, right_y;
        
        for (size_t i = 0; i < X.size(); i++) {
            if (X[i][split.feature_index] <= split.threshold) {
                left_X.push_back(X[i]);
                left_y.push_back(y[i]);
            } else {
                right_X.push_back(X[i]);
                right_y.push_back(y[i]);
            }
        }
        
        return {left_X, left_y, right_X, right_y};
    }
    
    Float64 predictSample(const vector<Float64>& sample, const TreeNode* node) const {
        if (node->is_leaf) {
            return node->value;
        }
        
        if (sample[node->feature_index] <= node->threshold) {
            return predictSample(sample, node->left.get());
        } else {
            return predictSample(sample, node->right.get());
        }
    }
    
    void printNode(const TreeNode* node, Int64 depth) const {
        string indent(depth * 2, ' ');
        if (node->is_leaf) {
            cout << indent << "Leaf: " << node->value << endl;
        } else {
            cout << indent << "Feature " << node->feature_index 
                 << " <= " << node->threshold << endl;
            cout << indent << "Left:" << endl;
            printNode(node->left.get(), depth + 1);
            cout << indent << "Right:" << endl;
            printNode(node->right.get(), depth + 1);
        }
    }
};

class ExtremeRandomForest {
private:
    vector<ExtremeDecisionTree> trees;
    Int64 n_estimators;
    Int64 max_samples;
    
public:
    ExtremeRandomForest(Int64 n_estimators = 100, Int64 max_samples = -1)
        : n_estimators(n_estimators), max_samples(max_samples) {}
    
    void fit(const vector<vector<Float64>>& X, const vector<Float64>& y) {
        PrecisionTimer timer;
        timer.start();
        
        trees.clear();
        trees.reserve(n_estimators);
        
        Int64 sample_size = max_samples > 0 ? max_samples : X.size() / 3;
        
        #pragma omp parallel for
        for (Int64 i = 0; i < n_estimators; i++) {
            // Bootstrap sampling
            auto [bootstrap_X, bootstrap_y] = bootstrapSample(X, y, sample_size);
            
            ExtremeDecisionTree tree;
            tree.fit(bootstrap_X, bootstrap_y);
            
            #pragma omp critical
            trees.push_back(move(tree));
        }
        
        double elapsed = timer.elapsed();
        cout << "=== EXTREME RANDOM FOREST ===" << endl;
        cout << "Trees: " << n_estimators << endl;
        cout << "Sample size: " << sample_size << endl;
        cout << "Training time: " << elapsed * 1000 << " ms" << endl;
        cout << "Speed: " << n_estimators / elapsed << " trees/sec" << endl;
    }
    
    Float64 predict(const vector<Float64>& sample) const {
        vector<Float64> predictions;
        for (const auto& tree : trees) {
            predictions.push_back(tree.predict(sample));
        }
        return HighPrecisionStats::mean_simd(predictions);
    }
    
    vector<Float64> predict_batch(const vector<vector<Float64>>& samples) const {
        vector<Float64> predictions(samples.size(), 0.0);
        
        #pragma omp parallel for
        for (Int64 i = 0; i < static_cast<Int64>(samples.size()); i++) {
            vector<Float64> tree_predictions;
            for (const auto& tree : trees) {
                tree_predictions.push_back(tree.predict(samples[i]));
            }
            predictions[i] = HighPrecisionStats::mean_simd(tree_predictions);
        }
        
        return predictions;
    }
    
private:
    pair<vector<vector<Float64>>, vector<Float64>>
    bootstrapSample(const vector<vector<Float64>>& X, const vector<Float64>& y, Int64 sample_size) {
        random_device rd;
        mt19937 gen(rd());
        uniform_int_distribution<Int64> dist(0, X.size() - 1);
        
        vector<vector<Float64>> bootstrap_X;
        vector<Float64> bootstrap_y;
        
        for (Int64 i = 0; i < sample_size; i++) {
            Int64 idx = dist(gen);
            bootstrap_X.push_back(X[idx]);
            bootstrap_y.push_back(y[idx]);
        }
        
        return {bootstrap_X, bootstrap_y};
    }
};

// High-performance neural network with SIMD activation functions
class ExtremeNeuralNetwork {
private:
    struct Layer {
        vector<vector<Float64>> weights;
        vector<Float64> biases;
        string activation;
        
        Layer(Int64 input_size, Int64 output_size, const string& activ = "relu")
            : activation(activ) {
            // He initialization
            Float64 stddev = sqrt(2.0 / input_size);
            random_device rd;
            mt19937 gen(rd());
            normal_distribution<Float64> dist(0.0, stddev);
            
            weights.resize(output_size, vector<Float64>(input_size));
            for (auto& row : weights) {
                for (auto& val : row) {
                    val = dist(gen);
                }
            }
            
            biases.resize(output_size, 0.1);
        }
    };
    
    vector<Layer> layers;
    Float64 learning_rate;
    PrecisionTimer timer;
    
public:
    ExtremeNeuralNetwork(Float64 lr = 0.001) : learning_rate(lr) {}
    
    void addLayer(Int64 input_size, Int64 output_size, const string& activation = "relu") {
        layers.emplace_back(input_size, output_size, activation);
    }
    
    void fit(const vector<vector<Float64>>& X, const vector<Float64>& y, 
             Int64 epochs = 100, Int64 batch_size = 32) {
        timer.start();
        
        for (Int64 epoch = 0; epoch < epochs; epoch++) {
            Float64 total_loss = 0.0;
            Int64 batches = 0;
            
            // Mini-batch training
            for (Int64 start = 0; start < static_cast<Int64>(X.size()); start += batch_size) {
                Int64 end = min(start + batch_size, static_cast<Int64>(X.size()));
                vector<vector<Float64>> batch_X(X.begin() + start, X.begin() + end);
                vector<Float64> batch_y(y.begin() + start, y.begin() + end);
                
                total_loss += trainBatch(batch_X, batch_y);
                batches++;
            }
            
            if (epoch % 10 == 0) {
                cout << "Epoch " << epoch << ", Loss: " << total_loss / batches << endl;
            }
        }
        
        double elapsed = timer.elapsed();
        cout << "=== EXTREME NEURAL NETWORK ===" << endl;
        cout << "Layers: " << layers.size() << endl;
        cout << "Epochs: " << epochs << endl;
        cout << "Batch size: " << batch_size << endl;
        cout << "Training time: " << elapsed << " seconds" << endl;
    }
    
    Float64 predict(const vector<Float64>& sample) {
        vector<Float64> activation = sample;
        for (const auto& layer : layers) {
            activation = forwardPass(activation, layer);
        }
        return activation[0]; // Single output
    }
    
private:
    Float64 trainBatch(const vector<vector<Float64>>& batch_X, 
                      const vector<Float64>& batch_y) {
        Float64 batch_loss = 0.0;
        
        for (size_t i = 0; i < batch_X.size(); i++) {
            // Forward pass
            vector<vector<Float64>> layer_activations = {batch_X[i]};
            for (const auto& layer : layers) {
                layer_activations.push_back(forwardPass(layer_activations.back(), layer));
            }
            
            // Calculate loss (MSE)
            Float64 prediction = layer_activations.back()[0];
            Float64 error = prediction - batch_y[i];
            batch_loss += error * error;
            
            // Backward pass (simplified)
            vector<Float64> gradients = {2.0 * error};
            for (Int64 layer_idx = layers.size() - 1; layer_idx >= 0; layer_idx--) {
                gradients = backwardPass(layer_activations[layer_idx], 
                                       layers[layer_idx], gradients);
            }
        }
        
        return batch_loss / batch_X.size();
    }
    
    vector<Float64> forwardPass(const vector<Float64>& input, const Layer& layer) {
        vector<Float64> output(layer.biases.size());
        
        // Matrix multiplication
        #pragma omp parallel for
        for (Int64 i = 0; i < static_cast<Int64>(layer.weights.size()); i++) {
            Float64 sum = layer.biases[i];
            for (size_t j = 0; j < input.size(); j++) {
                sum += layer.weights[i][j] * input[j];
            }
            output[i] = activate(sum, layer.activation);
        }
        
        return output;
    }
    
    vector<Float64> backwardPass(const vector<Float64>& input, Layer& layer, 
                               const vector<Float64>& grad_output) {
        vector<Float64> grad_input(input.size(), 0.0);
        
        // Update weights and biases
        for (size_t i = 0; i < layer.weights.size(); i++) {
            for (size_t j = 0; j < input.size(); j++) {
                Float64 grad = grad_output[i] * input[j];
                layer.weights[i][j] -= learning_rate * grad;
                grad_input[j] += layer.weights[i][j] * grad_output[i];
            }
            layer.biases[i] -= learning_rate * grad_output[i];
        }
        
        return grad_input;
    }
    
    Float64 activate(Float64 x, const string& activation) {
        if (activation == "relu") return max(0.0, x);
        if (activation == "sigmoid") return 1.0 / (1.0 + exp(-x));
        if (activation == "tanh") return tanh(x);
        return x; // linear
    }
};